# Import required PySpark modules
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType
from pyspark.sql.functions import to_date, when, col, lit, current_timestamp, from_utc_timestamp, row_number, expr
from datetime import datetime
from pyspark.sql.window import Window

# Create Spark session with event logging disabled
spark = SparkSession.builder \
    .appName('CSVReaderWithSchemas') \
    .config('spark.eventLog.enabled', 'false') \
    .getOrCreate()

# DB connection info
db_url = "jdbc:postgresql://aapsql-apm1012739-00dev01.cbogo6ayippz.us-east-2.rds.amazonaws.com:5432/postgres"
db_user = "src_mod_dev"
db_password = "{yy92K9Y_4FMZ,01"
db_driver = "org.postgresql.Driver"

def skip_first_and_last(df):
    w = Window.orderBy(lit(1))  # Arbitrary order, since CSV has no natural order
    df_with_rownum = df.withColumn("rownum", row_number().over(w))
    total_rows = df_with_rownum.count()
    # Keep rows where rownum > 1 and rownum < total_rows
    return df_with_rownum.filter((col("rownum") > 1) & (col("rownum") < total_rows)).drop("rownum")

def write_to_postgres(df, table_name):
    df.write \
    .format("jdbc") \
    .option("url", db_url) \
    .option("dbtable", table_name) \
    .option("user", db_user) \
    .option("password", db_password) \
    .option("driver", db_driver) \
    .option("autocommit", "true") \
    .mode("append") \
    .save()

# Read the pipe-delimited file without a header
df = spark.read.csv('c:/Users/AM35308/OneDrive - Elevance Health/Documents/2025/MOD/ISG Load Job/test2/pipe_case_input_one.csv', schema=isg_custom_schema, sep='|', header=False)
print("Original row count:", df.count())
df = skip_first_and_last(df)

grp_fields = [
    "GRP_NBR", "PROD_TYP", "ST_CD", "BRKR_NBR", "CNTRCT_TYP", "MIG_IND", "MBR_CNTRCT_ORG_EFF_DT",
    "MBR_CNTRCT_BGN_DT", "LST_BILL_NBR", "EMP_MBR", "EXCHG_IND", "CNTRCT_CNCL_ORG_EFF_DT",
    "PROD_TYP_NM", "SNDR_ID_QULFR", "SNDR_ID", "RCVR_ID_QULFR", "RCVR_ID", "CNTRL_NBR", "ACK_REQ",
    "FN_ID_CD", "SNDR_CD", "RCVR_CD", "GRP_CNTRL_NBR", "VER_ID_CD", "TRNCTN_SET_IDNTFR_CD",
    "TRNSCTN_SET_CNTRL_NBR", "TRNSCTN_VER", "TRNSCTN_SET_PRPS_CD", "REF_ID", "BILL_ENTY_EFF_DT",
    "PRE_EXST_END_DT", "PRE_EXST_MOS", "AIAN_IND", "GRP_PD_TO_DT", "MBR_CNTRCT_CAN_DT",
    "MBR_CNTRCT_DUE_DT", "MBR_CNTRCT_DLNQ_STS", "MTL_LVL_IND", "QHP_IND", "NEW_CMPNY_CD",
    "NEW_MBR", "NEW_FUND_TYP", "NEW_PROD_CD", "HIOS_VARN_ID", "HIOS_CNTRCT_CD", "HIOS_REF_CNTRCT_CD"
]

mem_fields = [
    "CERT_NBR", "MBR_HCID","MBR_SEQ", "MBR_CD", "MBR_CNTRCT_STS_CD", "PD_TO_DT", "MBR_CVRG_END_DT", "MBR_TERM_RSN",
    "PHN_NBR", "EMAIL", "FRST_NM", "MID_INTL", "LST_NM", "BRTH_DT", "MBR_SSN", "MBR_CAN_EFF_DT",
    "PED_DT", "SPKN_LANG", "WRT_LANG", "MBR_RACE", "MBR_ETHNCTY", "GNDR", "REL_CD", "CRTFCTN_CD",
    "MBR_TID", "MBR_EID", "LOCO_CD", "CA_OLD_CERT_NBR", "APTC_AMT", "MBR_EFF_DT", "MBR_ORG_EFF_DT",
    "EXCHG_SUB_ID", "EXCHG_MBR_ID"
]

mem_addr_fields = [
    "CERT_NBR", "MBR_HCID", "MBR_SEQ", "MBR_CD", "ADDR_TYP", "ADDR_CARE_OF", "ADDR_LN_1", "ADDR_LN_2", "ADDR_CITY", "ADDR_ST", "ADDR_ZIP"
]

mem_pcp_fields = [
    "CERT_NBR", "MBR_HCID", "MBR_SEQ", "MBR_CD", "PROV_ID", "PROV_LST_NM", "PROV_EFF_DT"
]

df_grp = df.select(grp_fields)
df_mem = df.select(mem_fields)
df_address = df.select(mem_addr_fields)
df_pcp = df.select(mem_pcp_fields)

write_to_postgres(df_grp, "modmbr.mod_poc_grp_ld")
write_to_postgres(df_mem, "modmbr.mod_poc_mem_ld")
write_to_postgres(df_address, "modmbr.mod_poc_mem_addr_ld")
write_to_postgres(df_pcp, "modmbr.mod_poc_mem_pcp_ld")

 